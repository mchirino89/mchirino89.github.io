---
layout: post
title:  "Unit tests are not silver bullets"
excerpt: Are automated tests really that necessary? If so, why aren't we writing more of them? Is TDD a cult?
categories: work
tags: [work, coding, rant, testing]
---

[referralLink]: https://www.barrykooij.com/wordsesh-presentation-unit-testing-wordpress/
 
[docPost]: {% post_url /swift/2021-05-06-documentation-in-practice %}
[testing]: {% post_url /swift/2020-11-02-testing3rdFwk %}
[vapor]: {% post_url /swift/2021-03-06-serverSideVapor %}

{:refdef: style="text-align: center;"}
![reflexive cat](/assets/posts/14_unitTesting/cover.jpg)
{: refdef}
{:refdef: style="text-align: center;"}
Photo from [Barry Kooij][referralLink]
{: refdef}

> You test suite can't fail if you don't any tests
> vs
> 100% test coverage is REQUIRED

Depending where you're standing, you could be more inclined toward one of the above extremes. Let's address the elephant in the room by saying neither one of them is true. 

## No tests, no problems 🤦🏽‍♂️

This has been pretty much the state of the art for the last number of years in all the software factories (at least the ones [I've worked on](https://www.linkedin.com/in/mauriciochirino/)) and I can tell for experience it's no fun at all. You end up shipping code with close to zero confidence whenever there's no a full __manual__ regression test. 

Accounting the scenario described above makes the entire CI/CD process unattainable due to software's exponential complexity. The more code we write, the more states' permutations we have to account. This is simply not an scalable endeavor by any measure without involving some sort of automation.

## Every single line of code HAS TO be covered 😰

Then there's the other end of the spectrum: 100% coverage dreamland. This goal might be something achievable whenever we're starting a project from scratch with the mindset it will reach production state no matter what AND will have a lifespan of months or longer. Think for real hard about those two conditions I just mentioned.

Most software projects start either as side hustle/hobby or MVP to validate market-fit as soon as possible. Suffice to say neither one of them have unit tests in its initial scope, nor it should be if you ask me. At such early stage, there's no point to heavily invest time in designing a robust architecture -let alone setting a test suite with unit, integration, E2E tests and so forth- without any certainty that the project will live long enough to reap the benefits of said investment.

![cost graph](/assets/posts/14_unitTesting/cost.jpg)
{:refdef: style="text-align: center;"}
Source: [great talk](https://youtu.be/TQ9rng6YFeY) about the economics of software design by J.B. Rainsberger
{: refdef}

Then, there is the real world. The harsh reality the vast majority of us face is that we inherit legacy code bases every time we enter a new job. Adding test coverage to an already productive software project is a noble but often futile effort since there's no real value to the business in it by itself.

## Weren't you in favor of testing Mauri, WTF? 🤨

> When a measure becomes a target, it ceases to be a good measure. – Goodhart's law

If you've been reading my blog for awhile now, all of this might sound as a hard left turn coming from me. I get it, but the reason I'm hammering the 100% test coverage absolute is because there's no real point in doing so. Let me explain:

As stated before, the more code we write the more complex our system become. _% of test coverage_ by itself is no guarantee of proper operation. Wrong incentives are often the source of many evils, for instance a more Jr developer might be tempted to write tests that add no real value nor cover any real use case only to keep coverage. This is a poor use of their time and it might be much better expended [documenting][docPost] or learning a [new framework][vapor] that might make them more productive



